{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13 - Clustering I\n",
    "Name: Andrew Hawk  \n",
    "Class: CSCI 349 - Intro to Data Mining  \n",
    "Semester: 2020SP  \n",
    "Instructor: Brain King  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, homogeneity_completeness_v_measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) [P] Copy the following code to build a very basic set of clustered data using the make_blobs function in\n",
    "sklearn.datasets.samples_generator\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y_true = make_blobs(n_samples=60, centers=5,\n",
    "cluster_std=(0.3,0.4,0.5,0.7,0.7),\n",
    "center_box=(0, 8), random_state=1234)\n",
    "2) [P] Convert your data into a single pandas data frame with three variables, \"x1\", \"x2\", and \"target\". The\n",
    "variable \"target\" will represent our ground truth, i.e. the correct cluster class. Be sure it is a true\n",
    "categorical variable. Show the first 5 observations, which should have two numeric variables, named \"x1\"\n",
    "and \"x2\", and one category, named \"target\".\n",
    "3) [P] Show the info() and a table of the counts of the \"target\" variable (you should have the same\n",
    "number of observations for each label)\n",
    "4) [P] Create a scatterplot of the data, using the target variable to color each cluster. You should have five\n",
    "colored clusters. Two classes should stand out as quite separate. Three will have some minor overlap.\n",
    "\n",
    "5) [P] Read the reference for KMeans: https://scikit-\n",
    "learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans\n",
    "\n",
    "Now, create a clustering using KMeans. Let's assume you have prior knowledge that there are indeed 5\n",
    "clusters, but you aren't aware of ground truth. Thus, you can only assess the SSE and silhouette coefficient for\n",
    "your clustering.\n",
    "Print the inertia_ (SSE) and the silhouette_score results for this clustering.\n",
    "6) [P] Generate a scatterplot again, however, now color the points based off of the labels generated by the\n",
    "clustering, and NOT the correct labels. Add the centroids using a distinguishing color and size with an\n",
    "alpha=0.5.\n",
    "Your plot should look something like the following:\n",
    "\n",
    "7) [P] Generate the same plots with a KMeans clustering with k=3 and k=4. You'll need this to explain your next\n",
    "steps.\n",
    "8) [P] Let's take a step backward and assume that you are uncertain about how many actual clusters you have.\n",
    "Evaluate cluster sizes from 2 through 10. Create a data frame that stores the SSE and the silhouette score for\n",
    "each k. Show the resulting data frame.\n",
    "9) [P] Show the elbow plot (i.e. a line plot) of both measures on separate graphs. Use k as the x axis.\n",
    "10) [M] What does the SSE suggest is the best value of k? What about the silhouette coefficient? Refer to your\n",
    "three plots above to help you explain what happened. Why do you think there are these discrepancies?\n",
    "(If you have the interest, you should go back and regenerate your data with much smaller cluster_std values...\n",
    "and you'll see much more consistent results! That tells you something about how well these techniques work\n",
    "when your data are not linearly separable, and when your clusters are not all equally distributed.)\n",
    "11) [P] Now, re-generate your clustering for k=5. You will likely notice that your labels will be different colors than\n",
    "your original labels. That's annoying. Write the code to remap your cluster labels to use the same order as\n",
    "your labels as your ground truth. And, re-generate the correct plot, AND the clustered plot. This will make it\n",
    "easier to distinguish the observations that have different labels between the plots.\n",
    "(HINT: How? Perhaps the easiest approach is to use the contingency_matrix function in\n",
    "sklearn.metrics.cluster package. Show the output of that, then think about how you can use that to\n",
    "get the best mapping.)\n",
    "If you do it correctly, you should have two plots that look very, very similar:\n",
    "Your plots might be slightly different, but you should see at least a couple of points that were partitioned into\n",
    "the wrong cluster.\n",
    "12) Re-generate your two plots above into a single plot that clearly highlights every point that has been assigned\n",
    "to an incorrect correct cluster. For example, here is one approach you could use:\n",
    "13) [M] Since we do have ground truth, there are several metrics that are used to assess the quality of your\n",
    "clustering. Read the section on clustering performance evaluation in Scikit-learn's documentation page:\n",
    "https://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation\n",
    "Pay close attention to the following: Adjusted Rand Index, Homogeneity, Completeness, and V-measure. In\n",
    "this section, very briefly describe each of these four measures. Though we did not explicitly cover them in class, these are good to know.\n",
    "14) [P] Create a new data frame that contains these four measures for all KMeans clusterings of k between 2-10.\n",
    "As before, show your data frame, then generate a plot for each. However, since each of these falls on the\n",
    "same scale, you could generate them on one plot, or show them as subplots.\n",
    "15) [P] OPTIONAL â€“ A benchmark dataset has been going around called the diamonds dataset. It's a rich dataset\n",
    "of 53940 rows over 10 variables. Each row represents various characteristics of diamonds. The primary\n",
    "motivation of these data is to challenge the machine learning community to predict the price of a diamond,\n",
    "based on the other 9 characteristics.\n",
    "Load in the data as follows:\n",
    "df_diamonds = sns.load_dataset(\"diamonds\")\n",
    "This page has good info about the variables: https://www.kaggle.com/shivam2503/diamonds\n",
    "Your aim right now is to simply assess the following - are there any natural clusterings of these data? If so,\n",
    "over which variables? How many clusters? (NOTE: If you are computing the silhouette over many values of k,\n",
    "this could take a bit of time.)\n",
    "(NOTE: don't spend too much time on this yet. The next lab will repeat this question, but allow you to use\n",
    "Hierarchical Agglomerative clustering to help you answer the question.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
